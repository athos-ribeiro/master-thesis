\chapter{Conclusions and Future Work}
\label{ch:conclusion}

In this work, we developed and described kiskadee, a continuous source code
static analysis tool that can monitor different software repositories and run
multiple static analyzers on different software package versions. To rank the
static analysis warnings in each report, based on the likelihood of a warning
being a true positive, we presented a novel approach in which we train a
prediction model to classify static analysis warnings from different static
analyzers. Our model achieved 0.805 accuracy, 0.678 precision, and 0.958 recall
when classifying our test data set.

Different from related works, kiskadee's ranking approach does not use features
based on the analyzed project intrinsic properties for model training, namely,
source code change history and code metrics. Consequently, by smoothly
decreasing the classification accuracy, the model obtained can be used
successfully with any given software project. This is a compelling trade-off to
build generic tools, like kiskadee, that can perform static analysis in any
project provided as input and arbitrate on the positiveness of the alarms
generated in this fashion, allowing the continuous monitoring and analysis of
different software repositories, such as the ones provided by
GNU/Linux distributions.

To answer our first research question (\textbf{RQ1}) of whether it is possible
to rank or classify warnings using a dataset composed by the warnings only,
i.e., without using the analyzed source code as input, we trained a predictive
model capable of classifying multiple static analyzers warnings into true or
false positives for any given software project (the features used in our
ranking approach include the number of warnings triggered for a file and the
name of the tools that triggered the warning). We then used our classifier to
rank the warnings from these multiple static analyzers into a single report in
a universal reporting notation. The ranking was organized by sorting the
warnings in the report according to the model prediction probabilities: the
ones with a higher likelihood of being actual software flaws are placed at the
top of the list, and the warnings more likely to be false positives are placed
at the bottom of the list. We showed how our approach compares to randomly
sorting the warnings in the report: results show our ranking method performs
over five times better than random ranking.

The methodology and tools used to build the data set of labeled static analysis
warnings answer our second research question (\textbf{RQ2}) of whether it is
possible to automate the labeling process. We were able to 
label $25959$ warnings without individually inspecting them. The only manual step
taken during the labeling process was comparing the tools warning messages with
the categories of software flaws present in Juliet to comply with the test suite
documentation and verify if a given warning was referring to the category of flaws
being tested at a given moment.

For full reproducibility of our ranking experiments, the data sets and files
used in this study, including the alarms generated by examining the Juliet test
suite with the different static analyzers used in this thesis, both in their
original forms and converted to the Firehose universal report format, are
publicly available at
\url{https://www.ime.usp.br/~athoscr/files/ranking_data.tgz}. These resources
can also aid the reader to better understand the input and output used in each
step described by our methodology.

While our ranking model does not offer the same performance as the related
works with the best numerical results for ranking or classifying static
analysis warnings, it comes close while offering a much more extensive horizon
of possible applications since we do not use project specific features to rank
the warnings. This means that by using our approach, an application does not
need to have any previous knowledge about the software projects that were
analyzed to generate the static analysis reports given as input to our model.

Supported by the ranking approach we developed, kiskadee can be used to reduce the
cost of inspecting false alarms by setting a minimum value for the rate in
which real flaws are found per inspection in a ranked list (i.e., a confidence
level). When the rate of real flaws per inspections drops below that level, one
could stop the inspection for that warning list. A variety of other tools to
aggregate and display warnings from multiple static analyzers are both
available in the industry and studied in the academia~\citep{buckers2017uav,
heinemann2014teamscale}. A promising application of the approach to rank static
analyzer warnings presented in this thesis is to integrate these tools, which
aggregate warnings from multiple static analyzers in dashboards, with the
techniques introduced here. 

kiskadee is licensed under the GNU Affero General Public License. Its
development repository, the complete project documentation, and the data set
used for the ranking experiments here presented are available at
\url{pagure.io/kiskadee}.

\section{Future Work}

Juliet was the only dataset used to perform our experiments. It is important to
find or build other datasets and include more static analyzers in the
experiments to make the results presented during this thesis more solid. One
possibility would be to manually build a first dataset by searching FLOSS
repositories for patches that fix Common Vulnerabilities and Exposures (CVE)
and tag that code section (before applying the patch) as buggy. Then, we could
look for methods to automate this labeling steps and automate the construction
of such dataset. This is left for future work.

It is possible to employ our ranking strategy to reduce the cost of
inspecting false alarms by setting a minimum value for the rate in which real
flaws are found per inspection in a ranked list (i.e., a confidence level),
when the rate of real flaws per inspections drops below that level, one could
stop the inspection for that warning list. Studying these confidence levels and
the trade-off between loss of information and the cost of inspecting a more significant
number of false alarms is left for future work.

Related works propose methods to identify bugs included or fixed in specific
software versions, as mentioned in Section~\ref{sub:related_work}.  Also, there
are existing tools like csdiff\footnote{\url{https://github.com/kdudka/csdiff}}
that provide some logic to parse static analyzer reports from different
software versions and list new and removed warnings.  These tools and
techniques could be used to filter warnings by version, so the ranked warning
list displayed to the user would be a subset of the whole analysis report
containing only the warnings introduced in a specific version of the analyzed
software, reducing the scope of the manual inspections performed by software
development teams.  This could also aid in identifying false positives, as
proposed by \cite{kim_which_2007} since false positives tend to
persist between software versions. As claimed by~\cite{spacco_tracking_2006}, tracking warnings across software versions is
not as trivial as it may seem at first glance. Hence, integrating such techniques
to kiskadee is left as future work.

Another interesting topic to be explored in future work is the study and design
of a universal static analysis report standard. While Firehose meets our
experimental needs both as a report format and as a tool with its parsers and
generators, there is no evidence of research effort or validation on the report
format design. Validating it or designing a new format validated by static
analysis tool developers, practitioners and researchers would be a valuable
contribution to the field.

Finally, during our experiments, we could observe the poor performance of some
open source static analyzers. NIST have evaluated several source code static
analyzers, both proprietary and open source, and published their results
through the Static Analysis Tool Exposition (SATE) workshop
reports~\citep{okun2013report}. Their results present much lower false positive
rates than the ones we found during our experiments. Although SATE reports do
not provide the names of the evaluated tools when reporting data such as false
positive rates, we believe that the proprietary tools outperform FLOSS tools on
the field. Studying the static analysis techniques used by FLOSS tools and
comparing them to the ones used by proprietary tools (when feasible) and to the
state of the art is left as the last topic for future work.

\section{Outlook}

Source code static analysis can be reduced to the halting problem, hence, it is
undecidable. This undecidability problem may drive researchers away from the
field, since one may argue that there is no motivation to work on an issue
known to be unsolvable. Recent studies have proven that applying different 
techniques to find heuristics to reduce the false positive rates of static
analyzers provide means to produce tools that can find software flaws with
false positive rates that are low enough to produce reports that are valuable
to software developers and industry.

Studying new techniques to reduce the amount of false positives on the static
analyzer level is a relevant field which still needs progress. Most of the
static analysis tools that can produce reports with low false positive
rates are proprietary tools and their internal techniques are not always
publicly available. Open source static analyzer developers should be as close
as possible from the academy so they can always test and implement new
techniques in production level tools to reduce the false positive rate gaps
that exist between proprietary and open source static analysis tools. Software
engineering practitioners should also look into static analysis as a technique
to improve code quality and prevent critical bugs to be released with
production software versions.

Finally, open source communities should embrace static analysis, preferably
with open source static analyzers, to both improve the quality of the code they
ship and to provide feedback to static analyzer projects so they can keep
improving their tools and techniques.
