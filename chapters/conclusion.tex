\chapter{Conclusions and Future Work}
\label{ch:conclusion}

In this work, we developed and described kiskadee, a continuous source code
static analysis tool that can monitor different software repositories and run
multiple static analyzers on different software package versions. To rank the
static analysis warnings in each report, based on the likelihood of a warning
being a true positive, we presented a novel approach in which we train a
prediction model to classify static analysis warnings from different static
analyzers. Our model achieved 0.805 accuracy, 0.678 precision, and 0.958 recall
when classifying our test data set.

Different from related works, kiskadee's ranking approach does not use features
based on the analyzed project intrinsic properties for model training, namely,
source code change history and code metrics. Consequently, by smoothly
decreasing the classification accuracy, the model obtained can be used
successfully with any given software project. This is a compelling trade-off to
build generic tools, like kiskadee, that can perform static analysis in any
project provided as input and arbitrate on the positiveness of the alarms
generated in this fashion, allowing the continuous monitoring and analysis of
different software repositories, such as the ones provided by
GNU/Linux distributions.

To answer our first research question (\textbf{RQ1}) of whether it is possible
to rank or classify warnings without using the analyzed source code as input,
we trained a predictive model capable of classifying multiple static analyzers
warnings into true or false positives for any given software project.  We then
used our classifier to rank the warnings from these multiple static analyzers
into a single report in a universal reporting notation. The ranking was
organized by sorting the warnings in the report according to the model
prediction probabilities: the ones with a higher likelihood of being actual
software flaws are placed at the top of the list, and the warnings more likely
to be false positives are placed at the bottom of the list. We showed how our
approach compares to randomly sorting the warnings in the report: results show
our ranking method performs over five times better than random ranking.

The methodology and tools used to build the data set of labeled static analysis
warnings answer our second research question (\textbf{RQ2}) of whether it is
possible to automate the labeling process: We were able to 
label $25959$ warnings without individually inspecting them. The only manual step
taken during the labeling process was comparing the tools warning messages with
the categories of software flaws present in Juliet to comply with the test suite
documentation and verify if a given warning was referring to the category of flaws
being tested at a given moment.

For full reproducibility of our ranking experiments, the data sets and files
used in this study, including the alarms generated by examining the Juliet test
suite with the different static analyzers used in this paper, both in their
original forms and converted to the Firehose universal report format, are
publicly available at
\url{https://www.ime.usp.br/~athoscr/files/ranking_data.tgz}.

While the ranking model here achieved does not offer the same performance as
the state of the art, it comes close while offering a much more extensive horizon of
possible applications.

Supported by the ranking approach developed, kiskadee can be used to reduce the
cost of inspecting false alarms by setting a minimum value for the rate in
which real flaws are found per inspection in a ranked list (i.e., a confidence
level). When the rate of real flaws per inspections drops below that level, one
could stop the inspection for that warning list. A variety of other tools to
aggregate and display warnings from multiple static analyzers are both
available in the industry and studied in the academia~\cite{buckers2017uav,
heinemann2014teamscale}. A promising application of the approach to rank static
analyzer warnings presented in this work is to integrate these tools, which
aggregate warnings from multiple static analyzers in dashboards, with the
techniques introduced here. 

It is possible to employ the ranking strategy presented to reduce the cost of
inspecting false alarms by setting a minimum value for the rate in which real
flaws are found per inspection in a ranked list (i.e., a confidence level),
when the rate of real flaws per inspections drops below that level, one could
stop the inspection for that warning list. Studying these confidence levels and
the trade-off between loss of information and the cost of inspecting a more significant
number of false alarms is left for future work.

Related works propose methods to identify bugs included or fixed in specific
software versions, as mentioned in Section~\ref{sub:related_work}.  Also, there
are existing tools like csdiff\footnote{\url{https://github.com/kdudka/csdiff}}
that provide some logic to parse static analyzer reports from different
software versions and list new and removed warnings.  These tools and
techniques could be used to filter warnings by version, so the ranked warning
list displayed to the user would be a subset of the whole analysis report
containing only the warnings introduced in a specific version of the analyzed
software, reducing the scope of the manual inspections performed by software
development teams.  This could also aid in identifying false positives, as
proposed by Kim et al.~\cite{kim_which_2007} since false positives tend to
persist between software versions. As claimed by Spacco et
al.~\cite{spacco_tracking_2006}, tracking warnings across software versions is
not as trivial as it may seem at first glance. Hence, integrating such techniques
to kiskadee is left as future work.

Another interesting topic to be explored in future work is the study and design
of a universal static analysis report standard. While Firehose meets our
experimental needs both as a report format and as a tool with its parsers and
generators, there is no evidence of research effort or validation on the report
format design. Validating it or designing a new format validated by static
analysis tool developers, practitioners and researchers would be a valuable
contribution to the field.

Finally, during our experiments, we could observe the poor performance of some
open source static analyzers. NIST have evaluated several source code static
analyzers, both proprietary and open source, and published their results
through the Static Analysis Tool Exposition (SATE) workshop
reports~\cite{okun2013report}. Their results present much lower false positive
rates than the ones we found during our experiments. Although SATE reports do
not provide the names of the evaluated tools when reporting data such as false
positive rates, we believe that the proprietary tools outperform FLOSS tools on
the field. Studying the static analysis techniques used by FLOSS tools and
comparing them to the ones used by proprietary tools (when feasible) and to the
state of the art is left as the last topic for future work.

kiskadee is licensed under the GNU Affero General Public License. Its
development repository, the complete project documentation, and the data set
used for the ranking experiments here presented are available at
\url{pagure.io/kiskadee}.
