\chapter{Conclusion}

In this paper, we described a novel approach in which we train a prediction
model to classify static analysis warnings from different tools. Our model
achieved 0.805 accuracy, 0.678 precision, and 0.958 recall when classifying our
test data set.  Although studies in the literature provide models with slightly
better performance than the one achieved by the model here proposed, our
approach does not use features based on the analyzed project internal
properties, namely, source code change history and code metrics. This means
that the model obtained can be used successfully with any given software
project, which is an interesting trade-off to build generic tools to be used as
entry points to automated source code static analysis.

To answer our first research question (\textbf{RQ1}) of whether it is possible to rank or classify warnings
without using the analyzed source code as input, we trained a predictive model
capable of classifying multiple static analyzers warnings into true or false
positives for any given software project.
We then used our classifier to rank the warnings from these multiple static
analyzers into a single report in a universal reporting notation. The ranking
was organized by sorting the warnings in the report according to the model
prediction probabilities: the ones with a higher likelihood of being actual
software flaws are placed at the top of the list, and the warnings more likely
to be false positives are placed at the bottom of the list. We showed how our
approach compares to randomly sorting the warnings in the report: results show
our ranking method performs over 5 times better than random ranking.

%While the model does not offer the
%same performance as the state of the art, it comes close while offering a much
%wider horizon of possible applications.

The methodology and tools used to build the data set of labeled static analysis
warnings answer our second research question (\textbf{RQ2}) of whether it is
possible to automate the labeling process: We were able to 
label $25959$ warnings without individually inspecting them. The only manual step
taken during the labeling process was comparing the tools warning messages with
the categories of software flaws present in Juliet to comply with the test suite
documentation and verify if a given warning was referring to the category of flaws
being tested at a given moment.

For full reproducibility, the data sets and files used in this study,
including the alarms generated by examining the Juliet test suite with the
different static analyzers used in this paper, both in their original forms and
converted to a universal report format, are publicly available at
\textbf{**Link omitted for double-blind review**}.

As future work, it is possible to employ the ranking strategy presented in this
paper to reduce the cost of inspecting false alarms by setting a minimum value
for the rate in which real flaws are found per inspection in a ranked list
(i.e., a confidence level), when the rate of real flaws per inspections drops
below that level, one could stop the inspection for that warning list. Studying
confidence levels and the trade-off between loss of information and the cost of
inspecting a larger number of false alarms is left for future works.

A variety of tools to aggregate and display warnings from multiple static
analyzers are both available in the industry and studied in the
academia~\cite{buckers2017uav, heinemann2014teamscale}. A promising application
of the approach to rank static analyzer warnings presented in this paper is to
integrate these tools (that aggregate warnings from multiple static analyzers
in dashboards) with the techniques introduced here. In this context, we are
developing a continuous static analysis tool, called \textbf{**BLIND REVIEW:
OMITTED NAME AND REFERENCE**}, that monitors software repositories for new
releases and runs multiple static analyzers on each new release of the
monitored software.  In another future work, we will present the integration of
the concepts presented in this paper with \textbf{**BLIND REVIEW: OMITTED
NAME**} tool.
