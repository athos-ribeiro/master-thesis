\chapter{Conclusion}
\label{ch:conclusion}

In this paper, we described a novel approach in which we train a prediction
model to classify static analysis warnings from different tools. Our model
achieved 0.805 accuracy, 0.678 precision, and 0.958 recall when classifying our
test data set.  Although studies in the literature provide models with slightly
better performance than the one achieved by the model here proposed, our
approach does not use features based on the analyzed project internal
properties, namely, source code change history and code metrics. This means
that the model obtained can be used successfully with any given software
project, which is an interesting trade-off to build generic tools to be used as
entry points to automated source code static analysis.

To answer our first research question (\textbf{RQ1}) of whether it is possible to rank or classify warnings
without using the analyzed source code as input, we trained a predictive model
capable of classifying multiple static analyzers warnings into true or false
positives for any given software project.
We then used our classifier to rank the warnings from these multiple static
analyzers into a single report in a universal reporting notation. The ranking
was organized by sorting the warnings in the report according to the model
prediction probabilities: the ones with a higher likelihood of being actual
software flaws are placed at the top of the list, and the warnings more likely
to be false positives are placed at the bottom of the list. We showed how our
approach compares to randomly sorting the warnings in the report: results show
our ranking method performs over 5 times better than random ranking.

%While the model does not offer the
%same performance as the state of the art, it comes close while offering a much
%wider horizon of possible applications.

The methodology and tools used to build the data set of labeled static analysis
warnings answer our second research question (\textbf{RQ2}) of whether it is
possible to automate the labeling process: We were able to 
label $25959$ warnings without individually inspecting them. The only manual step
taken during the labeling process was comparing the tools warning messages with
the categories of software flaws present in Juliet to comply with the test suite
documentation and verify if a given warning was referring to the category of flaws
being tested at a given moment.

For full reproducibility, the data sets and files used in this study,
including the alarms generated by examining the Juliet test suite with the
different static analyzers used in this paper, both in their original forms and
converted to a universal report format, are publicly available at
\textbf{**Link omitted for double-blind review**}.

As future work, it is possible to employ the ranking strategy presented in this
paper to reduce the cost of inspecting false alarms by setting a minimum value
for the rate in which real flaws are found per inspection in a ranked list
(i.e., a confidence level), when the rate of real flaws per inspections drops
below that level, one could stop the inspection for that warning list. Studying
confidence levels and the trade-off between loss of information and the cost of
inspecting a larger number of false alarms is left for future works.

A variety of tools to aggregate and display warnings from multiple static
analyzers are both available in the industry and studied in the
academia~\cite{buckers2017uav, heinemann2014teamscale}. A promising application
of the approach to rank static analyzer warnings presented in this paper is to
integrate these tools (that aggregate warnings from multiple static analyzers
in dashboards) with the techniques introduced here. In this context, we are
developing a continuous static analysis tool, called \textbf{**BLIND REVIEW:
OMITTED NAME AND REFERENCE**}, that monitors software repositories for new
releases and runs multiple static analyzers on each new release of the
monitored software.  In another future work, we will present the integration of
the concepts presented in this paper with \textbf{**BLIND REVIEW: OMITTED
NAME**} tool.

%\section{Visualization}
%\label{sec:visualization}

Related works propose methods to identify bugs included or fixed in specific
software versions, as mentioned in Section \ref{history_review}.
Also, there are existing tools like
csdiff\footnote{\url{https://github.com/kdudka/csdiff}} that provide some
logic to parse static analyzer reports from different software versions and
list new and removed warnings. csdiff provides parsers for both cppcheck and
the Clang Static Analyzer; a parser for Frama-C would have to be written.

These tools and techniques could be used to filter warnings by version, so the
ranked warning list displayed to the user will be a subset of the whole
analysis report, containing only the warnings introduced in a specific version
of the analyzed software. This could also aid in identifying false positives,
as proposed by Kim et al.~\cite{kim_which_2007}, since the false positives are
not fixed between versions.

For our visualization tool, we will assess the effort needed to patch csdiff
to suit our needs and the effort to reproduce related works. Then we will
apply the technique that requires less effort to identify the warnings introduced
in a specific version of a package.

% etado da análise estática com software livre = ruim

%%%%%%
% start: conclusao do paper do kiskadee
%%%%%%

Different from related works, kiskadee's ranking approach does not use
features based on the analyzed project intrinsic properties for model training,
namely, source code change history and code metrics. Consequently, by
smoothly decreasing the classification accuracy, the model obtained can be used
successfully with any given software project. This is a compelling trade-off
to enable kiskadee to analyze and rank any project given as input, allowing the
continuous monitoring and analysis of different software repositories, such as
the ones provided by GNU/Linux distributions.

kiskadee can be used to reduce the cost of inspecting false alarms by setting a
minimum value for the rate in which real flaws are found per inspection in a
ranked list (i.e., a confidence level). When the rate of real flaws per
inspections drops below that level, one could stop the inspection for that
warning list. Interesting future works include studying confidence levels and
the trade-off between loss of information and the cost of inspecting a larger
number of false alarms.

kiskadee is licensed under the GNU Affero General Public License. Its
development repository, the complete project documentation, and the data set
used for the ranking experiments here presented are available at
\url{pagure.io/kiskadee}.
