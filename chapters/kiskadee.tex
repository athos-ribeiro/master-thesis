\chapter{Continuous Static Analysis Tool Design}
\label{ch:kiskadee}

In Free/Libre and Open Source Software (FLOSS) projects, a
common source of bug reports are the GNU/Linux distributions.
These distributions ship thousands of software projects, which
they call packages. Distribution developers refer to the
projects that maintain the software they ship in the
distribution as \textit{upstream}.  It is not unusual for
distribution developers to report bugs in upstream projects or
to send patches to fix bugs found by their distribution users
or during the packaging process. Figure
\ref{fig:floss_bugs_workflow} shows a common bug reporting
workflow, where users report the bugs they find to the
distribution developers, who in turn report these bugs (in
addition to bugs found during the packaging process) to the
upstream project maintainers of the package in which bugs were
found.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.60\textwidth]{figures/floss_bugs_workflow} 
  \caption{FLOSS bug report workflow.}
  \label{fig:floss_bugs_workflow} 
\end{figure}

By continuously running multiple static analyzers in several of these packages,
i.e., once for each version of the package, we can create a database of static
analysis reports on software projects of different sizes and application
domains. Developers can then use the information in this database to find and
act on software flaws.

We chose to use GNU/Linux distributions due to the high amount
of software packages available and the well-defined and
documented interfaces they provide to download the latest
versions of these packages. It is also an advantage that the
cultural norm for GNU/Linux distribution developers is to
report (and often propose fixes for) bugs. Therefore, using
their repositories for this work may provide a broader user
base for the tools and techniques developed in this research.

\section{Features and System Overview}
\label{sec:system_overview}

\begin{figure}[hbt]
\centering
\includegraphics[width=.7\textwidth]{figures/kiskadee-overview.pdf}
  \caption{kiskadee design overview.}\label{fig:kiskadee:overview}
\end{figure}

To continuously run static analysis on software packages and handle the false
alarms generated by these analyses, we developed \textit{kiskadee}. Its main features are described below:

\begin{enumerate}
\item \textbf{Monitor specific software repositories for new releases.} This\
way, we can track and analyze each release of the monitored packages since we\
started monitoring them.

\item \textbf{Download new software packages from the repositories.}

\item \textbf{Run multiple static analyzers on the downloaded packages.}

\item \textbf{Translate each static analyzer report to a common report format.} This\
is needed to enable analyzing the reports during the ranking and visualization\
phase. Since the system is extensible, we do not want to write parsing\
rules for each static analyzer report format to compare the analyzers, so we do\
this before saving the static analysis output.

\item \textbf{Rank warnings.} The warnings are ranked based on their probablilities of being true of false positives. The methodology and techniques applied to implement this feature are described in Chapter~\ref{ch:ranking_warnings}.

\item \textbf{Save the reports in a database.}

\item \textbf{Make data available through an API.}

\item \textbf{Present data to users on a web user interface.} Users are able to filter warnings per package name and version.
\end{enumerate}

Figure~\ref{fig:kiskadee:overview} represents kiskadee's architecture overview,
where the numbers denote its execution flow.
\begin{itemize}
  \item In steps (1) and (2), kiskadee
monitors software repositories for new releases.
    \item In step (3), kiskadee
downloads the source code of each new software version in a repository it
monitors and runs a set of predefined static analyzers on it in step (4).
    \item In step (5),
kiskadee translates each static analyzer report to a common warnings report format.
This common format is needed because each static analyzer defines
its unique format to report warnings.
    \item In step (6), kiskadee ranks the warnings
based on their probability of being real bugs, where warnings on the top have a
higher probability of being real bugs and warnings on the bottom are more
likely to be false positives. This ranking step is performed with a
classification model, described in Section~\ref{sec:ranking}.
    \item The ranked
warnings are then saved in a database in step (7), using kiskadee's common
warning report format.  
    \item Finally, in step (8), kiskadee provides an API consumed
by a visualization tool to display the ranked warnings filtered by package
versions. The information provided can be used either by distribution developers to
evaluate and report possible bugs upstream or by upstream developers
themselves. Researchers can also use kiskadee's database in different contexts.
\end{itemize}

The next sections describe important details related to kiskadee features.
Appendix~\ref{ape:implementation} provides more details on kiskadee's architecture and
implementation details.

\section{Monitoring Software Repositories}
\label{sec:monitoring}

Although pointing kiskadee to GNU/Linux distribution repositories may have
advantages, most distributions do not package every single new software version
for the packages it distributes. Some versions are not packaged for different
reasons. For instance, some projects claim that they only provide stable
software every other release. Other projects release versions too fast, making
the effort of packaging every single release pointless to the distribution
packager.

Fedora Project uses an external system to monitor the software projects they
distribute. Anitya \cite{anitya} maps upstream projects to distribution package
names. Whenever a new version of an upstream project is released, Anitya
publishes the new release in a Fedora infrastructure publish-subscribe system
where other systems in the distribution infrastructure can handle it.  New
software versions are published by Anitya as soon as they are released
upstream. Analyzing these new software versions with kiskadee as early as they
are released allows the distribution developers to address potential bugs found
by kiskadee before the software is shipped to final users. It also avoids
missing the releases that are not packaged in the distribution for the reasons
aforementioned. Therefore, kiskadee monitors packages by reading information
published by Anitya in the Fedora infrastructure publish-subscribe system.

kiskadee can monitor other software repositories as well through its plugin
architecture (kiskadee's fetchers). Each fetcher must implement functions that
define which repository to monitor, how to monitor it, and which static
analyzers to run for that repository. Hence, we can extend \textit{kiskadee} to
run different static analyzers for different software repositories or GNU/Linux
distributions.

\section{Static Analyzers}
\label{sec:analyzers}

The NIST Software Assurance Metrics and Tool Evaluation (SAMATE) project
provides a list of \textit{source code security
analyzers}\footnote{\url{samate.nist.gov/index.php/Source_Code_Security_Analyzers.html}},
which they define as static analyzers that \textit{examine source code to
detect and report weaknesses that can lead to security vulnerabilities}.

% arrumar frase para relacioná-la com a sessão de rankeamento,
% dizer que a ferramenta é extensível e que podemos plugar outros analizadores
\textit{kiskadee} was run with three static analysis tools to generate the
warnings data set for our experiments. The Criteria to select the tools were:
\begin{enumerate}
  \item \textbf{The tools must be able to examine C/C++ code for security flaws (e.g., buffer overflows, null pointer dereferences).} Different programming languages have different problems and we do not want to address this variable at the moment. Since software systems are often developed in the C/C++ programming languages and several software flaws are mapped under Common Weakness Enumeration (CWE) for these languages, we chose to work with these languages.
  \item the tool source code must be released under a FLOSS license.
\end{enumerate}
Criterion~1 ensures the tool can analyze a subset of the test cases in our data
set, introduced in Section~\ref{sec:ranking}, whereas criterion 2 preserves us
from disputes by tool vendors regarding the analysis of the results (such as
allegations of sub-optimal tool calibration or detrimental calculation
methodology). FLOSS tools also simplify the process of retrieving string
constructs for static analyzer warning messages and categories when necessary,
since we can verify their source code.

Following the criteria, the static
analyzers selected were \emph{Clang Static Analyzer} (version 3.9.1),
\emph{Cppcheck} (version 1.79), and \emph{Frama-C} (version 1.14 with the value
analysis plugin activated).

\textit{Frama-C} with its value analysis plug-in was the first choice, since it
was the tool used for the Ockham Criteria~\cite{black_sate_2016} and the
authors claim Frama-C is a sound tool, not producing false positives, which may
help on the ranking phase of this study. It uses abstract interpretation
techniques. Although Frama-C is not listed in the SAMATE recommended tools, it
is recommended by the Flawfinder
website\footnote{\url{dwheeler.com/flawfinder}}, one of the secondary tool
lists recommended by SAMATE. Next \textit{cppcheck} was chosen, which is a
static analyzer that works by matching on tokens, without deeper analysis
methods. This tool authors also claim to avoid generating false positives.
Finally, \textit{Clang Static Analyzer} works by performing inter-procedural
analysis on the source code and, differently from the other two selected tools,
the authors do not claim that this tool does not generate false positives.

\section{Common Reporting Language}
\label{sec:firehose}

%%%
The problem of having a common reporting language for static analyzers was
discussed in a previous work. By applying the Ockham Sound Analysis
Criteria~\cite{black_sate_2016}, we designed a simplified common bug report
format to compare known bugs with bug reports from the tools evaluated with the
criteria. The criteria data and programs to reproduce results are available in
the SATE V Ockham Sound Analysis Criteria NIST Internal Report
\cite{black_sate_2016}, where the bug report formats specified can be retrieved
from and replicated. While the format was fit to perform the analysis, it was
designed on top of the criteria concepts and the tools used for the criteria,
which needed the following information:

\begin{itemize}
\item a specific location for the bug;
\item the type of the bug being reported (related to a CWE);
\item a code snippet with the buggy code;
\item information to identify false alarms on that code snippet.
\end{itemize}

To extract information from the used test suite (code snippets with known
flaws injected) to verify the soundness of the analyzed tools, an XML
format was defined. Listing~\ref{lst:ockham_xml} is an example of the format,
extracted from one of the files with Ockham results.  Although tools to extract
information from the format were developed and are available, no formal
definition of the format was described, since this format was used for this
specific version of the criteria.

\begin{lstlisting}[caption={Ockham Criteria XML bug report example},label={lst:ockham_xml}]
<?xml version='1.0' encoding='utf-8'?>
<sites>
  <file name="CWE369_Divide_by_Zero__int_fgets_divide_51b.c">
    <site type="div_by_zero" line="27" info="data" val="buggy">
      <code><![CDATA[printIntLine(100 / data);]]></code>
    </site>
    <site type="div_by_zero" line="38" info="data" val="good">
      <code><![CDATA[printIntLine(100 / data);]]></code>
    </site>
    <site type="div_by_zero" line="47" info="data" val="good">
      <code><![CDATA[printIntLine(100 / data);]]></code>
    </site>
  </file>
</sites>
\end{lstlisting}

To compare the findings of the evaluated tools with actual data from the test suite,
tool reports were parsed and converted to a very simple comma-separated values
(CSV) format containing \textit{file name}, \textit{line} and \textit{bug category
related to a CWE}. An example of this CSV format, extracted from the Ockham
Criteria results, is shown in Listing~\ref{lst:ockham_csv}.

\lstset{language=Python}
\begin{lstlisting}[caption={Ockham Criteria CSV bug report example},label={lst:ockham_csv}]
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_01.c, 43, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_01.c, 62, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_02.c, 45, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_02.c, 73, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_02.c, 92, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_03.c, 45, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_03.c, 73, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_03.c, 92, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_04.c, 51, divByZero
CWE121_Stack_Based_Buffer_Overflow__char_type_overrun_memcpy_04.c, 79, divByZero
\end{lstlisting}

To rank warnings, we would like to have information on analyzed software
versions, which static analyzer performed the analysis, the version of the
analyzer, severity of the problem found, and the string message given by the
analyzer. All this information may be interesting for research or bug reporting
purposes. Thus, if we were to use the format we developed for the Ockham Criteria,
changes would be needed, both to the format and to the tools provided to work
with it.

%%%
FLOSS development communities have been discussing a common static analysis
report output format as well. The Fedora Project Static Analysis Special Interest Group
\cite{fedora:static:sig} designed a tool to run static analyzers during the
package build process \cite{fedora:mockwithanalysis}.  Although the tool itself
is in its early development stages and not ready for usage in production, the
developers discussed \cite{fedora:mlist} a common report format for the static
analyzers in their mailing lists. After a few iterations, with Debian Project
developers collaboration, they created \textit{Firehose}, a complete definition
of a common warnings report format for static analyzers and a set of tools to
generate, parse, and verify this format.
\textit{Firehose} addresses the issues previously discussed, as analyzer name
and version, optional fields for severity, and complete warning messages. It also allows custom fields in the format, which we use to include kiskadee's ranking information.
Appendix \ref{apx:firehose} is a RELAX-NG schema describing the
\textit{firehose} format.
We use \textit{Firehose} as kiskadee's
common warning report format, eliminating the need to design a new format and
to develop the tools to handle it, like parsers and generators.

%%% GSoC, new parsers, etc...

\section{API and Web User Interface}
\label{sec:api}

\section{Related Projects}
\label{sec:related_projects}

\subsection{mock-with-analysis}

\subsection{Debile}
Debian Project has been developing a tool to run multiple static analyzers in
Debian repositories and return the reports in the \textit{firehose}
format\footnote{\url{debile.debian.net}}.  The tool is still in early development
(not in production), but we intend to use part of it with \textit{kiskadee} to
avoid replicating their work. Some development on it to provide options to

\subsection{Coala}

