\chapter{A Static Analysis Warnings Ranking Model}
\label{ch:ranking}

We approach the problem of ranking multiple static analyzer warnings with a
supervised learning method to arbitrate about the warnings positiveness. To
learn from observed examples, we need to obtain and process a data set of static
analysis warnings labeled as true or false positive. We describe the steps taken
in this direction in the next section.

\section{Data Extraction}
\label{sec:data}

To build a predictive model as discussed, we need to obtain good and sufficient
data to train it. The first step we take in this direction is choosing both the
source code to analyze and the tools with which to analyze it. Then, to
adequately process the output of multiple tools, we need to normalize the
information provided by them. We approach this in two levels: at the first
level, we translate alarms from the different tools into a unified format and
apply labels to each warning to distinguish them into either true positives or
false positives. At the second level, we associate each warning to specific
features that will compose a data set to train our prediction model.

\subsection{Choosing Data Sources}

A data set of labeled static analysis warnings may be obtained by running
static analyzers on previously selected source code and matching the
triggered warnings with actual software defects, labeling the warnings as true
or false positives. The source code used for extracting the data set may
consist of real-world software or synthetic test cases, i.e., programs written
with intentional defects.

Previous studies have used real-world software to arbitrate about the
positiveness of static analysis warnings~\cite{kremenek2004correlation,
jung2005taming, yoon2014reducing, yuksel2013classification,
ruthruff_predicting_2008}. A relevant obstacle for using real-world software to
retrieve a data set is that one must meticulously inspect each alarm triggered
by a static analyzer and the relevant location in the analyzed program to
determine whether the alarm is a true or a false positive.  Unless one has
access to an environment where alarms positiveness information is already
available, as in Ruthruff~\cite{ruthruff_predicting_2008}, performing these
manual inspections is an overly time-consuming approach, which must be
performed for each different static analysis tool being assessed. Differently,
the location and other details of the injected flaws in synthetic test cases
are previously known, facilitating studies and freeing researchers from
performing expensive searches along their data sets.

We use Juliet~\cite{juliet} version 1.2 to generate our data set. Juliet is a
synthetic C/C++ test suite created by the United States National Security
Agency (NSA) and distributed by the National Institute of Standards and
Technology (NIST) under the public domain. It is composed of 61,387 test cases
covering 118 different software security-related flaw categories. Each test
case is a code section with an instance of a specific flaw (to catch true
positives) and an additional section with a correct, fixed instance of that
previous flawed section (to catch false positives).  Juliet also includes a
user guide with instructions on how to assess and label static analysis tool
warnings generated over its test cases.

We ran three static analysis tools to generate the data set for our
experiments. Here we use the same static analyzers integrated into
kiskadee since their output will compose kiskadee final
static analysis reports, which we want to rank with our model.
While the Criteria employed to select the tools are described
in Section~\ref{sec:analyzers}, the list of selected static
analyzers is shown in Table~\ref{tab:selected_tools}.

After choosing Juliet and the analysis tools, we must consider how to combine
them to build our data set. Let $\{J\}$ be the whole set of Juliet test cases
and $\{T_x\}$ the subset of Juliet test cases that tool $x$ is able to detect.
Figure~\ref{fig:venn} shows a Venn diagram for the situation where three static
analysis tools are employed. We may choose to process $\{J\}$, $\{T_1 \cup
T_2\cup\dots T_n\}$, or $\{T_1 \cap T_2 \cap\dots T_n\}$. We may even imagine
processing, with each tool $x$, only the subset of test cases $\{T_x\}$ that
the tool is able to detect.

\input{figures/venn.tex}

In our case, there is no difference
between the first and second options: since we are dealing with
true and false positives only, and not negatives, $\{J - (T_1 \cup T_2\cup\dots T_n)\}$
(region represented in Figure~\ref{fig:venn_out})
is never included in the results. There might be some difference,
however, in studies focusing on negatives.

\input{figures/venn_out.tex}

Similary, there is no difference between processing different test
subsets with each tool and processing $\{T_1 \cup T_2\cup\dots T_n\}$:
test cases that cannot be addressed by some tool will not be in
both cases. Again, focusing on negatives would invalidate this.

The only truly different approach, therefore, is to analyze only $\{T_1 \cap
T_2 \cap\dots T_n\}$ (region represented in Figure~\ref{fig:venn_common}). This
has several drawbacks. The model we want to build works by identifying
correlations among data points. With this choice of subset, we would exclude
data in $\{(T_1 \cap T_2) - T_3\}$ etc., shown in Figure~\ref{fig:venn_exp}, reducing the strength of the
algorithm. This would also prevent us from using the correlations obtained to
rank warnings thrown by a single tool.  Since ranking is a very important
aspect of our work, this would be highly inappropriate.

\input{figures/venn_common.tex}

\input{figures/venn_exp.tex}

In light of the previous discussion, we consider the use
of $\{J\}$ as the simplest approach. As discussed, this does
not mean we are adding false or noisy data into the
model: some data points may be more or less relevant to the
classifier, depending on which and how many tools brought
them up, but identifying this relevance is exactly the
role of the classifier.

However, before examining Juliet with the selected static analysis tools, we still prune the
test suite to prevent analysis of test cases that depend on constructs of
specific operating systems or external libraries.
Table~\ref{tab:juliet_numbers} shows the total number of test cases in Juliet
before pruning warnings and the number of test cases after the pruning step for
both C and C++. The latter are the tests examined by the static analyzers for
alarms generation, consisting of 39,100 C/C++ test cases.

  \begin{table}
    \begin{center}
        \begin{tabular}{crr}\hline
          & Before Pruning & After Pruning \\
        \hline
          C & 36,078 & 22,459 \\
          C++ & 25,309 & 16,641 \\
          Total & 61,387 & 39,100 \\ \hline
        \end{tabular}
        \caption{Number of Juliet test cases}\label{tab:juliet_numbers}
    \end{center}
\end{table}

\subsection{Collecting Labeled Warnings From Multiple Static Analyzers}\label{sec:data:labels}

Based on Juliet 1.2 documentation, we process each file in the remaining test
cases to produce a list (L) with information on whether a static analysis
warning for a given location should be labeled as true positive or false
positive. If a static analyzer generates an alarm for a location not included
in this list or for a category not covered by the flaws present in that location, we do not
draw any conclusions about such location, since it would require thorough
manual inspection. Instead, we discard that warning. Then, we run each static
analyzer on the pruned test suite to generate the static analyses reports.
Table~\ref{tab:unlabeled_warnings} shows the total number of warnings
generated, before discarding the warnings whose labeling step cannot be
automated.

  \begin{table}
    \begin{center}
        \begin{tabular}{rr}\hline
          Tool & Warnings \\
        \hline
          Clang Static Analyzer & 37,229 \\
          Cppcheck & 124,025 \\
          Frama-C & 120,573 \\
          Total & 281,827 \\ \hline
        \end{tabular}
        \caption{Total number of warnings generated per tool}\label{tab:unlabeled_warnings}
    \end{center}
\end{table}

Seeing that each tool has its unique report format, we reduce the effort of
parsing reports by converting them all to a unified report (UR) in a common
format. Here, we use the Firehose format, introduced in
Chapter~\ref{ch:kiskadee}.  By matching each possible warning produced by the
static analyzers with the corresponding flaw categories covered by Juliet, we
can use the list L to produce labels for each warning in UR.

The set of labeled warnings can finally be examined to have a training set
extracted from it, as discussed in the next section.
Table~\ref{tab:labeled_warnings} summarizes the findings of the static
analyzers on the test cases, including the number of true and false positives
generated (TP and FP, respectively), the false positive rates, and the precision
for each tool and for UR, the aggregated reports composed of the warnings of
all tools.

  \begin{table}
    \begin{center}
        \begin{tabular}{crrrrr}\hline
          Tool & Warnings & TP & FP & FP Rate & Precision \\
        \hline
          Clang Analyzer & 6207 & 984 & 5223 & 0.84 & 0.16 \\
          Cppcheck & 4035 & 314 & 3721 & 0.92 & 0.08 \\
          Frama-C & 15717 & 8892 & 6825 & 0.43 & 0.57 \\
          \textbf{Aggregated tools} & 25959 & 10190 & 15769 & \textbf{0.61}  & \textbf{0.39} \\ \hline
        \end{tabular}
        \caption{Labeled warnings per tool}\label{tab:labeled_warnings}
    \end{center}
\end{table}

% discussão de taxa de falsos positivos e taxa de warnings utilizados deve ser icluída em resultados?

\subsection{Extracting Features from Labeled Warnings}\label{sec:data:features}

%With the analysis reports obtained by running the static analyzers on the set
%of known security flaws, we can empirically select the features that we will
%use for our model training. Although warning messages are different in each
%static analyzer since there are no standards for warning messages and not all
%of these tools adopt CWE terminology, it is possible to associate warnings
%triggered by the same software flaw, even if it is by using simple heuristics,
%like the location of each tool finding, which could also be used as a feature.

We obtain the features used to train our classifier from the set of labeled
warnings. Here, a feature is any characteristic that can be attributed to a
warning in the data set. To select relevant features for false and positive
alarm classification, we refer to previous studies that also rely on
characteristics extracted from alarms and source code to classify alarms
\cite{ruthruff_predicting_2008, kremenek2004correlation, heckman2009model,
jung2005taming}. For instance, Kremenek et al. \cite{kremenek2004correlation}
demonstrate that the tool warning positiveness is highly correlated to code
locality.  However, we only use the data extracted from the warnings
themselves, not relying on other information, such as the analyzed project
change history, size and complexity metrics, or artifact names (e.g., file name,
function name, package name). By not collecting such information, we are can
produce a more generic model applicable to different contexts without the
need to perform the expensive training step for each project one may want to
analyze.

We extract our set of features by processing the aggregated report of labeled
warnings.
While we can infer the name of the tool that triggered a warning, the programming
language analyzed, and the severity of the warning by looking
at a single warning at a time,
other features require processing the whole
aggregated report to be extracted. Namely, these are (1) the number of times the same
location was pointed as flawed in the report, (2) the number of warnings triggered
around the location of a given warning (e.g., warnings for locations at most 3
lines away from the current warning), (3) the category of the software flaw
suggested by the warning, (4) which other static analyzers generated warnings for
the same location, and (5) the number of warnings generated for the same file the
current warning is pointing to.

Except for warning category and warning severity, the features
mentioned above can be readily obtained for a given warning by analyzing the
whole aggregated report. We assign a category to a warning by matching its
message with one of the categories in Table~\ref{tab:warning_categories}. Since
Juliet test cases cover a wide range of software flaw categories and each
static analyzer can detect a specific set of bug categories (necessarily mapped
to Juliet categories), this list can be extended, breaking categories into more
specific subcategories to seek improvement in the importance of this feature in
the prediction model.

\begin{table}
  \begin{center}
      \begin{tabular}{ll}\hline
        Category name & Description \\
      \hline
        buffer &  Buffer related issues (e.g., Buffer overrun) \\
        overflow & Integer overflow and underflow \\
        div0 & Divisions by zero \\
        uninitvar & Uninitialized variable \\
        unusedvar & Unused variable \\
        pointer & Pointer issues (e.g., Null pointer dereference) \\
        operator & Misused operators (e.g., $if(myVar = buf[i])\{\}$) \\
        funcparams & Issues with function parameter \\
        alwaysbool & Expression is either always true or always false \\
        memory & Memory issues (e.g., Memory leak, Double free)\\
        other & Warnings not fit to the categories above \\ \hline
      \end{tabular}
      \caption{Possible Assigned Warning Categories}\label{tab:warning_categories}
  \end{center}
\end{table}

To assign a severity to the warnings, we normalize the severities of each tool
to values between 0 and 5. We assume that 5 refers to errors in the source
code, like syntax errors that would prevent the source code to be compiled; 4
refers to potential flaws; 3 and lower values refer to style or similar issues.
Thus, If a tool does not provide severity information, we assume 4. The
following list summarizes the features obtained from the labeled aggregated
report.

\begin{itemize}
  \item \textbf{Tool name:} Name of the tool that generated the warning;
  \item \textbf{Number warnings in the same file:} Number of warnings found in the same file as the warning in question, regardless of what tool generated it;
  \item \textbf{Category:} Category of the warning, as shown in Table~\ref{tab:warning_categories};
  \item \textbf{Redundancy level:} Number of warnings triggered for that same line, regardless of what tool generated it;
  \item \textbf{Number of neighbors:} Number of warnings less than 4 lines away from the triggered warning. Does not include the ones counted in Redundancy level;
  \item Finally, we include one \textbf{boolean feature} for each of the static analyzers to indicate if that tool triggered a warning for that location. We understand that these features help confirm software flaws and take advantage of the fact that each tool performs differently for specific cases.
\end{itemize}

% If we get any space left:
% mostrar alguns números sobre as features?
  % quantidade por categoria;
  % media ou gráfico com warnings por arquivo

\section{Arbitrating on Warnings Positiveness}
\label{sec:positiveness}

Given the data collected using the methodology presented in the previous
section, we now build a prediction model to classify each triggered warning as
being a true positive or a false positive. The classification results may also be
used to rank the warnings, as we describe in the second part of this section.

\subsection{Training Decision Trees with AdaBoost}

To make our model applicable to a broader variety of projects in a generic
fashion, we only use data extracted from the warnings themselves. Since we do
not inspect the source code or project history, which are shown to be the best
places to look for features when classifying source code static analysis
warnings as true or false positives, we turn to ensemble learning methods to
train several weak classifiers with our feature set. These weak classifiers
combined can then arbitrate on new examples together, composing a stronger
classifier.

A weak classifier performs slightly better than a random
classifier, i.e., $\epsilon < 0.5$ where $\epsilon$ is the classification error
of the classifier. Ensemble learning methods combine the prediction of weak
classifiers to build stronger classifiers with lower errors~\cite{aima}. One
widely used ensemble learning method is boosting~\cite{aima}, whose main idea is to run a
weak learning algorithm several times in different distributions of the
training set to generate various weak classifiers to be combined into a
stronger one. For this study, we chose to use the AdaBoost algorithm~\cite{freund1999short}, a more general version of the original
boosting algorithm~\cite{polikar2006ensemble}.

AdaBoost works by calling a base learner algorithm (\textbf{h}) repeatedly
(\textbf{T} times) while maintaining a set of weights over the training data
set. The weights are used to select a subset of the training data set to serve
as input for the base learner algorithm in a given iteration of AdaBoost
(higher weights mean a higher likelihood of an example being selected for an
iteration). Each example in the training data set starts with the same weight,
but they are updated after each iteration: the weights of misclassified
examples are increased and the weights of examples correctly classified are
decreased. This forces the base learner algorithm to focus on the hard examples
of the training data set~\cite{freund1999short}. Each weak classifier is
assigned a voting weight (\boldmath$\alpha$) based on its importance. The
strong binary classifier (\textbf{H}) is given by Eq.~(\ref{eq:ada}), where
\textbf{x} is a new example to be classified.

% if space permits, we could add the complete adaboost algorithm here
\begin{equation}\label{eq:ada}
  H\left(x\right) = \mathit{sign} \left( \sum\limits_{t=1}^T\alpha_t h_t(x) \right)
\end{equation}

The AdaBoost algorithm works with any given base learner. We use a decision
tree learning algorithm as our base learner because we have both categorical
and non-categorical features in our data set, as shown in
Section~\ref{sec:data}, and decision trees can work with both, without the need
to pre-process the data set. Furthermore, as shown in the literature, decision trees 
perform well with AdaBoost~\cite{drucker1996boosting}.

We divide our data set (the list of labeled features obtained in
Section~\ref{sec:data}) into a training set and a test set. The training set is
built by randomly selecting 75\% of the examples labeled as true positives and
75\% of the examples labeled as false positives from the features data set. We
then proceed to train our predictive model using 10-fold cross-validation
with the training set. This cross-validation consists in dividing a data set
into 10 groups: 9 groups are used to train the model while the remaining group is
used to test it. The training step is repeated ten times, switching the group
used for testing until all groups were used in this way.  We perform the
10-fold cross-validation technique with different values for \textbf{T} in
AdaBoost. We then compare the average performance of the classifiers obtained for
each different value of \textbf{T} validated in this manner and use the best
model trained during the cross-validation for that \textbf{T} to classify the
test set.

\subsection{Ranking Static Analysis Warnings}
\label{subsec:ranking}

We use the model trained as described in this chapter to rank the warnings in a
static analysis report based on the model classification probabilities. We
reorder the warnings in a list according to the probability given by our
classification model of the warning being a true positive, where warnings with
higher probabilities are ranked in the top of the list and warnings with lower
probabilities of being true positives are arranged in the bottom of the list.
This way, a programmer inspecting the ranked static analysis report may examine
only the top warnings in the list up to a given threshold, assuming a certain
risk of missing true positives. Alternatively, he/she may stop inspecting
warnings when false positives start to abound.
